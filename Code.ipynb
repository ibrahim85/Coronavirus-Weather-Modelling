{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "from SEIR import corona_seir_model_population\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'VJga0fAn'\n",
    "blacklist = []\n",
    "retry_set = set()\n",
    "STATIONS = pd.read_csv('data/stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_vals(df,axis='both',subset=[]):\n",
    "    '''\n",
    "    Drops columns with all\n",
    "    nan values from a given \n",
    "    data frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame for which\n",
    "        columns are to be\n",
    "        dropped.\n",
    "        \n",
    "    axis : str\n",
    "        Drops all rows with\n",
    "        nan if axis=rows,\n",
    "        all columns if axis=columns,\n",
    "        and both if axis=both.\n",
    "        \n",
    "    subset : list of str\n",
    "        For all columns in\n",
    "        subset, remove the\n",
    "        NaN rows.\n",
    "    '''\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    assert(isinstance(axis,str))\n",
    "    assert(isinstance(subset,list))\n",
    "    assert(isinstance(col,str) for col in subset)\n",
    "    \n",
    "    df = df.dropna(subset=subset)\n",
    "    \n",
    "    if(axis=='rows'):\n",
    "        df = df.dropna(how='all',axis=0)\n",
    "    elif(axis=='columns'):\n",
    "        df = df.dropna(how='all',axis=1)\n",
    "    elif(axis=='both'):\n",
    "        df = df.dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getWeatherData(latitude,longitude):\n",
    "    '''\n",
    "    Returns temperature \n",
    "    and humidity data \n",
    "    as per the latitude \n",
    "    and longitude entered.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "    \n",
    "    station = getNearbyStation(latitude,longitude)\n",
    "    if(station is None):\n",
    "        return\n",
    "    else:\n",
    "        print(\"Station found for (%f,%f):\\n%s\"%(latitude,longitude,station))\n",
    "    url = createHourlyURL(station,START_DATE,END_DATE)\n",
    "    weather_data = dict()\n",
    "    print(\"URL for getting weather data for (%f,%f):\\n%s\"%(latitude,longitude,url))\n",
    "    response = requests.get(url)\n",
    "    code = response.status_code\n",
    "    print(\"Got response, status = %f\"%(code))\n",
    "    try:\n",
    "        df=pd.DataFrame(response.json()['data'])\n",
    "        date_list = list(df['time_local'].unique())\n",
    "#         print(retry_set)\n",
    "        retry_set.discard((latitude,longitude))\n",
    "        for date in date_list:\n",
    "            mean_temp=df[df['time_local']==date]['temperature'].mean()\n",
    "            mean_humidity=df[df['time_local']==date]['humidity'].mean()\n",
    "            df_dict = dict()\n",
    "            df_dict['temperature'] = mean_temp\n",
    "            df_dict['humidity'] = mean_humidity\n",
    "            weather_data[date] = df_dict\n",
    "    except:\n",
    "        print('No data found for (%f,%f)'%(latitude,longitude))\n",
    "        print(\"Retry has to be done for (%f,%f)\"%(latitude,longitude))\n",
    "        coordinates = (latitude,longitude)\n",
    "        retry_set.add(coordinates)\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "def getNearbyStation(latitude,longitude):\n",
    "    '''\n",
    "    Given the latitude and\n",
    "    longitude of a area,\n",
    "    returns the nearest station\n",
    "    to it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "\n",
    "    url = createStationURL(latitude,longitude)\n",
    "\n",
    "    try:\n",
    "        filter1 = stations['Latitude']==latitude\n",
    "        filter2 = stations['Longitude']==longitude\n",
    "        return stations.where(filter1 & filter2).dropna(how='all')['Weather Station ID'][0]\n",
    "    except:\n",
    "        try:\n",
    "            print('Station does not exist in \"Stations.csv\", fetching from api...')\n",
    "            response = requests.get(url)\n",
    "            code = response.status_code\n",
    "            result = response.json()['data'][0]['id']\n",
    "            df = pd.DataFrame({\"Weather Station ID\":[result], \"Latitude\":[latitude], \"Longitude\":[longitude]})\n",
    "            STATIONS.append(df, ignore_index=True)\n",
    "            print('Station fetched for (%f,%f) is: %s'%(latitude,longitude,result))\n",
    "            return result\n",
    "        except Exception as ex:\n",
    "            print('No station found for (%f,%f). URL: %s'%(latitude,longitude,url))\n",
    "            print('exception: ',ex)\n",
    "            if(code == 403):\n",
    "                print(\"Retry has to be done for (%f,%f)\"%(latitude,longitude))\n",
    "                retry_set.add((latitude,longitude))\n",
    "            return\n",
    "        \n",
    "def createStationURL(latitude,longitude):\n",
    "    '''\n",
    "    Returns station URL\n",
    "    for given latitude\n",
    "    and longitude.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "    \n",
    "    return 'https://api.meteostat.net/v1/stations/nearby?lat='+str(latitude)+'&lon='+str(longitude)+'&limit=1&key='+API_KEY\n",
    "\n",
    "def createHourlyURL(station_id,start_date,end_date):\n",
    "    '''\n",
    "    Creates weather URL\n",
    "    for given station,\n",
    "    start date and end\n",
    "    date.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    station_id : str\n",
    "        Station id of the\n",
    "        region for which\n",
    "        data is to be fetched\n",
    "        \n",
    "    start_date : str\n",
    "        Date starting from which\n",
    "        data is to be fetched.\n",
    "        \n",
    "    end_date : str\n",
    "        Date ending at which\n",
    "        data is to be fetched\n",
    "    '''\n",
    "    assert(isinstance(station_id,str))\n",
    "    assert(isinstance(start_date,str))\n",
    "    assert(isinstance(end_date,str))\n",
    "    \n",
    "    url = 'https://api.meteostat.net/v1/history/hourly?station='+station_id+'&start='+start_date+'&end='+end_date+'&time_zone=Europe/London&time_format=Y-m-d&key='+API_KEY\n",
    "\n",
    "    return url\n",
    "\n",
    "def getCompleteWeatherData(coordinate_list,complete_weather_data=dict(),flag=True):\n",
    "    '''\n",
    "    Returns consolidated weather\n",
    "    data for all the coordinates\n",
    "    in list of coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coordinate_list : list(tuple)\n",
    "        List of coordinates to\n",
    "        be evaluated.\n",
    "        \n",
    "    complete_weather_data : dict\n",
    "        Map of coordinate to\n",
    "        weather data.\n",
    "    \n",
    "    flag : bool\n",
    "        If True, means some\n",
    "       coordinate_listts failed, retry\n",
    "        the failed requests.\n",
    "    '''\n",
    "    assert(isinstance(flag,bool))\n",
    "    assert(isinstance(complete_weather_data,dict))\n",
    "    assert(isinstance(coordinate_list,list))\n",
    "    \n",
    "    try:\n",
    "        if(flag==True):\n",
    "            flag=False\n",
    "            for lat_long in lat_long_list:\n",
    "                coordinate = (str(lat_long[0]),str(lat_long[1]))\n",
    "                if((coordinate not in complete_weather_data) and (coordinate not in blacklist)):\n",
    "                    flag = True\n",
    "                    complete_weather_data[(str(lat_long[0]),str(lat_long[1]))] = getWeatherData(lat_long[0],lat_long[1])\n",
    "                    time.sleep(10)\n",
    "            return getCompleteWeatherData(coordinate_list,complete_weather_data,flag)\n",
    "        else:\n",
    "            return complete_weather_data\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return complete_weather_data\n",
    "\n",
    "def getIndexByRegion(province,country,df):\n",
    "    '''\n",
    "    Gets index of a region\n",
    "    from a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        index is to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        index is to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        index is to be fetched\n",
    "    '''\n",
    "    assert(isinstance(country,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    if(type(province)!=str and np.isnan(province)):\n",
    "        idx = df[(df['Province/State'].isnull()) & (df['Country/Region']==country)].index\n",
    "        return idx.to_list()[0]\n",
    "    else:\n",
    "        idx = df[(df['Province/State']==province) & (df['Country/Region']==country)].index\n",
    "        return idx.to_list()[0]\n",
    "\n",
    "def fetch_province_country_by_region(region):\n",
    "    '''\n",
    "    Given a region as\n",
    "    Province,Country,\n",
    "    returns the province\n",
    "    and country or just\n",
    "    the country if no province\n",
    "    of the region.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : str\n",
    "        Region to be parsed\n",
    "    '''\n",
    "    assert(isinstance(region,str))\n",
    "    \n",
    "    result = region.split(\",\")\n",
    "    if(len(result)==2):\n",
    "        return result[0],result[1].strip()\n",
    "    else:\n",
    "        return np.NaN,region\n",
    "    \n",
    "def getPopulationByRegion(province,country,df):\n",
    "    '''\n",
    "    Given the province and\n",
    "    country of a region,\n",
    "    fetches the population\n",
    "    from the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        population is to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        population is to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        population is to be fetched\n",
    "    \n",
    "    '''\n",
    "    assert(isinstance(country,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    if(type(province)!=str and np.isnan(province)):\n",
    "        population = df[(df['Province/State'].isnull()) & (df['Country/Region']==country)].Population\n",
    "        return population.to_list()[0]\n",
    "    else:\n",
    "        population = df[(df['Province/State']==province) & (df['Country/Region']==country)].Population\n",
    "        return population.to_list()[0]\n",
    "    \n",
    "def getDatesByRegion(region,df):\n",
    "    '''\n",
    "    Given the province and\n",
    "    country of a region,\n",
    "    fetches the infection\n",
    "    duration from the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        dates are to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        dates are to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        dates are to be fetched\n",
    "    '''\n",
    "    assert(isinstance(region,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    region_row = df[df['Region']==region]\n",
    "    start_date = region_row['Time series start'].to_list()[0]\n",
    "    stop_date = region_row['Time series end'].to_list()[0]\n",
    "    \n",
    "    return start_date,stop_date\n",
    "\n",
    "def getLearningRate(region,df):\n",
    "    '''\n",
    "    Given the province and\n",
    "    country of a region,\n",
    "    fetches the learning\n",
    "    rate from the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        learning rate is\n",
    "        to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        learning rate is\n",
    "        to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        learning rate is\n",
    "        to be fetched\n",
    "    '''\n",
    "    assert(isinstance(region,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    region_row = df[df['Region']==region]\n",
    "    learning_rate = region_row['lr'].to_list()\n",
    "    print(\"Learning rate:\",learning_rate)\n",
    "    if(math.isnan(learning_rate[0])):\n",
    "        print(\"learning rate is null. returning 0.0004\")\n",
    "        return 0.0004\n",
    "    else:\n",
    "        return learning_rate[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. covid_19_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sno - Serial number<br/>\n",
    "ObservationDate - Date of the observation in MM/DD/YYYY<br/>\n",
    "Province/State - Province or state of the observation (Could be empty when missing)<br/>\n",
    "Country/Region - Country of observation<br/>\n",
    "Last Update - Time in UTC at which the row is updated for the given province or country. (Not standardised and so please clean before using it)<br/>\n",
    "Confirmed - Cumulative number of confirmed cases till that date<br/>\n",
    "Deaths - Cumulative number of of deaths till that date<br/>\n",
    "Recovered - Cumulative number of recovered cases till that date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. COVID_open_line_list_data.csv and COVID19_line_list_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual level data information\n",
    "<br/><br/>\n",
    "\n",
    "ID<br/>\n",
    "age<br/>\n",
    "sex<br/>\n",
    "city<br/>\n",
    "province<br/>\n",
    "country<br/>\n",
    "wuhan(0)_not_wuhan(1)<br/>\n",
    "latitude<br/>\n",
    "longitude<br/>\n",
    "geo_resolution<br/>\n",
    "date_onset_symptoms<br/>\n",
    "date_admission_hospital<br/>\n",
    "date_confirmation<br/>\n",
    "symptoms<br/>\n",
    "lives_in_Wuhan<br/>\n",
    "travel_history_dates<br/>\n",
    "travel_history_location<br/>\n",
    "reported_market_exposure<br/>\n",
    "additional_information<br/>\n",
    "chronic_disease_binary<br/>\n",
    "chronic_disease<br/>\n",
    "source<br/>\n",
    "sequence_available<br/>\n",
    "outcome<br/>\n",
    "date_death_or_discharge<br/>\n",
    "notes_for_discussion<br/>\n",
    "location<br/>\n",
    "admin3<br/>\n",
    "admin2<br/>\n",
    "admin1<br/>\n",
    "country_new<br/>\n",
    "admin_id<br/>\n",
    "data_moderator_initials<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region wise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_wise_data = drop_null_vals(pd.read_csv('data/covid_19_data.csv'),axis=\"both\")\n",
    "region_wise_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_line_list = drop_null_vals(pd.read_csv('data/COVID19_open_line_list.csv'),axis='both')\n",
    "open_line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list = drop_null_vals(pd.read_csv('data/COVID19_line_list_data.csv'),'both')\n",
    "line_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data (John Hopkins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "#     confirmed_url = 'https://raw.githubusercontent.com/y1singh/Coronavirus-Weather-Modelling/master/data/time_series_covid_19_confirmed.csv'\n",
    "    deaths_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "#     deaths_url='https://raw.githubusercontent.com/y1singh/Coronavirus-Weather-Modelling/master/data/time_series_covid_19_deaths.csv'\n",
    "    recovered_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "#     recovered_url = 'https://raw.githubusercontent.com/y1singh/Coronavirus-Weather-Modelling/master/data/time_series_covid_19_recovered.csv'\n",
    "    \n",
    "    print('Fetching confirmed data from git...')\n",
    "    time_series_confirmed = drop_null_vals(pd.read_csv(confirmed_url,error_bad_lines=False))\n",
    "    print('Fetched confirmed data from git')\n",
    "    \n",
    "    time_series_confirmed.to_csv('data/time_series_covid_19_confirmed.csv')\n",
    "    \n",
    "    time_series_deaths = drop_null_vals(pd.read_csv(deaths_url,error_bad_lines=False))\n",
    "    time_series_deaths.to_csv('data/time_series_covid_19_deaths.csv')\n",
    "    print('Fetched deaths data from git')\n",
    "    \n",
    "    time_series_recovered = drop_null_vals(pd.read_csv(recovered_url,error_bad_lines=False))\n",
    "    time_series_recovered.to_csv('data/time_series_covid_19_recovered.csv')\n",
    "    print('Fetched recovered data from git')\n",
    "    \n",
    "except:\n",
    "    # data not able to be fetched from git, fetching from local system\n",
    "    print(\"Data not able to fetch from git. Using local filesystem.\")\n",
    "    time_series_confirmed = drop_null_vals(pd.read_csv('data/time_series_covid_19_confirmed.csv'))\n",
    "    time_series_deaths = drop_null_vals(pd.read_csv('data/time_series_covid_19_deaths.csv'))\n",
    "    time_series_recovered = drop_null_vals(pd.read_csv('data/time_series_covid_19_recovered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = pd.read_csv(\"data/population_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping today's data\n",
    "time_series_confirmed.drop(time_series_confirmed.columns[len(time_series_confirmed.columns)-1], axis=1, inplace=True)\n",
    "time_series_deaths.drop(time_series_deaths.columns[len(time_series_deaths.columns)-1], axis=1, inplace=True)\n",
    "time_series_recovered.drop(time_series_recovered.columns[len(time_series_recovered.columns)-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_confirmed_coord = drop_null_vals(time_series_confirmed,subset=['Lat','Long'])\n",
    "series_confirmed_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and humidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_list = list(series_confirmed_coord[['Lat','Long']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>API Sample Calls</u></b>\n",
    "\n",
    "API to fetch station: https://api.meteostat.net/v1/stations/nearby?lat=1.283&lon=103.83&limit=1&key=XXXXXXX<br/>\n",
    "API to fetch daily historical data(not used any more): https://api.meteostat.net/v1/history/daily?station=10637&start=2017-01-01&end=2017-12-31&key=XXXXXXXX<br/>\n",
    "API to fetch historical hourly data: https://api.meteostat.net/v1/history/hourly?station=03772&start=2019-05-02&end=2019-05-11&time_zone=Europe/London&time_format=Y-m-d%20H:i&key=XXXXXXXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = str(datetime.strptime(series_confirmed_coord.columns[4], '%m/%d/%y').date())\n",
    "END_DATE = str(datetime.strptime(series_confirmed_coord.columns[-1], '%m/%d/%y').date())\n",
    "complete_weather_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lat_long in lat_long_list:\n",
    "    if(str(lat_long[0])+str(lat_long[1]) not in complete_weather_data):\n",
    "        complete_weather_data[str(lat_long[0])+str(lat_long[1])] = getWeatherData(lat_long[0],lat_long[1])\n",
    "        time.sleep(60)\n",
    "\n",
    "complete_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = getCompleteWeatherData(lat_long_list)\n",
    "STATIONS.to_csv(r'data\\stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for key in list(temp.keys()):\n",
    "    new_dict[str(key[0])+\",\"+str(key[1])] = temp[key]\n",
    "    \n",
    "my_dict = pd.DataFrame(new_dict).dropna(how=\"all\",axis=1).to_dict()\n",
    "with open('data/weather_mapping_data.json', 'w') as json_file:\n",
    "    json.dump(my_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_column = time_series_confirmed.columns[len(time_series_confirmed.columns)-1]\n",
    "sorted_series=time_series_confirmed.sort_values(by=last_column,ascending=False)\n",
    "sorted_series = sorted_series[sorted_series[last_column]>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_series.where(sorted_series['Country/Region'] == country,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_data_population(model,gt_infected,population,region,count,df):\n",
    "\tT_max = gt_infected.shape[0]-1\n",
    "\tmax_infected = np.max(gt_infected)\n",
    "\tdt = 1\n",
    "\tT = np.linspace(0,T_max,int(T_max/dt)+1).astype(int)\n",
    "\tN = population\n",
    "\tinit_exposed = int(gt_infected[0]*2)\n",
    "\tinit_vals = (N-init_exposed-gt_infected[0]),init_exposed,gt_infected[0],0\n",
    "\tgamma2 = 0.03\n",
    "\trho = 1.0\n",
    "\ttotal_epochs = 20000\n",
    "\t# lr = 0.04\n",
    "\t# lrd = 0.01\n",
    "# \tlr = 0.0004/max_infected\n",
    "\tlr = getLearningRate(region,df)/max_infected\n",
    "\tprint(\"learning rate: \",lr)\n",
    "\tlrd = 0.000\n",
    "\tcurr_params = 0.2,0.5,0.0,gamma2\n",
    "\tloss_arr = []\n",
    "\talpha_arr = []\n",
    "\tbeta_arr = []\n",
    "\tgamma1_arr = []\n",
    "\tgamma2_arr = []\n",
    "\tfor epoch in tqdm(range(total_epochs)):\n",
    "\t\tcurr_lr = lr/(1+epoch*lrd)\n",
    "\t\tloss_jacobian = epoch_fit_params_corona_seir_population(init_vals,curr_params,T,gt_infected,lr=curr_lr)\n",
    "\t\tloss_epoch = np.sum(loss_jacobian[0])\n",
    "\t\tnew_alpha = max(0,curr_params[0]+np.sum(loss_jacobian[1]))\n",
    "\t\tnew_beta = max(0,curr_params[1]+np.sum(loss_jacobian[2]))\n",
    "\t\tnew_gamma1 = max(0,curr_params[2]+np.sum(loss_jacobian[3]))\n",
    "\t\tnew_gamma2 = max(0,curr_params[3]+np.sum(loss_jacobian[4]))\n",
    "\t\tcurr_params = new_alpha,new_beta,new_gamma1,new_gamma2\n",
    "\t\tloss_arr.append(loss_epoch)\n",
    "\t\talpha_arr.append(new_alpha)\n",
    "\t\tbeta_arr.append(new_beta)\n",
    "\t\tgamma1_arr.append(new_gamma1)\n",
    "\t\tgamma2_arr.append(new_gamma2)\n",
    "\tbest_epoch = np.argmin(np.array(loss_arr))\n",
    "\tprint(\"Best learned params: {} {} {}\".format(alpha_arr[best_epoch],beta_arr[best_epoch],gamma1_arr[best_epoch]))\n",
    "\tplt.figure(count)\n",
    "\tplt.subplot(221)\n",
    "\tplt.axvline(x=best_epoch,color='k',linestyle='--')\n",
    "\tplt.plot(list(range(total_epochs)),loss_arr)\n",
    "\tplt.ylabel('Total MSE loss')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.subplot(222)\n",
    "\tplt.axvline(x=best_epoch,color='k',linestyle='--')\n",
    "\tplt.plot(list(range(total_epochs)),alpha_arr,label='alpha')\n",
    "\tplt.plot(list(range(total_epochs)),beta_arr,label='beta')\n",
    "\tplt.plot(list(range(total_epochs)),gamma1_arr,label='gamma1')\n",
    "\t# plt.plot(list(range(total_epochs)),gamma2_arr,label='gamma2')\n",
    "\tplt.title('Learning trajectory for '+region)\n",
    "\tplt.ylabel('Parameter value')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.legend()\n",
    "\t# print(init_vals)\n",
    "\tbest_params = alpha_arr[best_epoch],beta_arr[best_epoch],gamma1_arr[best_epoch],gamma2\n",
    "\tT_pred = np.linspace(0,10+T_max,int((10+T_max)/dt)+1).astype(int)\n",
    "\tlearned_results = model(init_vals,best_params,T_pred)\n",
    "\t# plt.figure(2)\n",
    "\tplt.subplot(212)\n",
    "\t# p = plt.plot(T,sim_results[0],label='GT Susceptible')\n",
    "\t# p = plt.plot(T_pred,learned_results[0]/N,linestyle='--',label='Predicted Susceptible')\n",
    "\t# p = plt.plot(T,sim_results[1],label='GT Exposed')\n",
    "\t# p = plt.plot(T_pred,learned_results[1]/N,linestyle='--',label='Predicted Exposed')\n",
    "\tp = plt.plot(gt_infected[T]/N,label='GT Infected')\n",
    "\tplt.plot(T_pred,learned_results[2]/N,color=p[0].get_color(),linestyle='--',label='Predicted Infected')\n",
    "\tprint(\"error percentage: \",abs(100*(learned_results[2][len(gt_infected)-1]-gt_infected[-1])/gt_infected[-1]))\n",
    "\t# p = plt.plot(T,sim_results[3],label='Recovered')\n",
    "\tp = plt.plot(T_pred,learned_results[3]/N,linestyle='--',label='Predicted Recovered')\n",
    "\tplt.legend()\n",
    "\tplt.ylabel('Fraction of population')\n",
    "\tplt.xlabel('Time (days)')\n",
    "\tplt.title('GT and learned models')\n",
    "\tplt.show()\n",
    "    \n",
    "def epoch_fit_params_corona_seir_population(init_vals, init_params, T, infected, lr=1e-2):\n",
    "\tmax_infected = np.max(infected)\n",
    "\tS0,E0,I0,R0 = init_vals\n",
    "\tN = S0+E0+I0+R0\n",
    "\tS, E, I , R = [S0], [E0], [I0], [R0]\n",
    "\tloss = [0]\n",
    "\talpha, beta, gamma1, gamma2 = init_params\n",
    "\tdt = T[1]-T[0]\n",
    "\tjacobian_mat = np.zeros((3,4))\n",
    "\t# updated_alpha, updated_beta, updated_gamma1, updated_gamma2 = [alpha],[beta],[gamma1],[gamma2]\n",
    "\tupdate_alpha, update_beta, update_gamma1, update_gamma2 = [0],[0],[0],[0]\n",
    "\t# print(infected)\n",
    "\tfor idx,t in enumerate(T[1:-3]):\n",
    "\t\t# print(\"{} {} {} {} {:.3f} {:.3f} {:.3f} {:3.3f}\".format(t,alpha,beta,gamma1,S[-1],E[-1],I[-1],np.max(jacobian_mat)))\n",
    "\t\tupdate_mat = np.array([[(1-beta*E[-1]/N),(-beta*S[-1]/N),0],[(beta*E[-1]/N),(1+beta*S[-1]/N-alpha-gamma1),0],[0,alpha,(1-gamma2)]])\n",
    "\t\tadd_mat = np.array([[0,(-S[-1]*(E[-1]/N)),0,0],[(-E[-1]),S[-1]*(E[-1]/N),(-E[-1]),0],[E[-1],0,0,(-I[-1])]])/max_infected\n",
    "\t\tjacobian_mat = np.matmul(update_mat,jacobian_mat)+add_mat\n",
    "\t\tmin_val = N\n",
    "\t\tS1 = S[-1] - (beta*S[-1]*E[-1]/N)*dt\n",
    "\t\tif S1<min_val: min_val=S1\n",
    "\t\tE1 = E[-1] + (beta*S[-1]*E[-1]/N - alpha*E[-1] - gamma1*E[-1])*dt\n",
    "\t\tif E1<min_val: min_val=E1\n",
    "\t\tI1 = I[-1] + (alpha*E[-1] - gamma2*I[-1])*dt\n",
    "\t\tif I1<min_val: min_val=I1\n",
    "\t\tR1 = R[-1] + (gamma1*E[-1] + gamma2*I[-1])*dt\n",
    "\t\tif R1<min_val: min_val=R1\n",
    "\t\tN1 = S1+E1+I1+R1-4*min_val\n",
    "\t\tS1 = N*(S1-min_val)/N1\n",
    "\t\tE1 = N*(E1-min_val)/N1\n",
    "\t\tI1 = N*(I1-min_val)/N1\n",
    "\t\tR1 = N*(R1-min_val)/N1\n",
    "\t\tS.append(S1)\n",
    "\t\tE.append(E1)\n",
    "\t\tI.append(I1)\n",
    "\t\tR.append(R1)\n",
    "\t\t# print(T[idx+1],infected[T[idx+1]])\n",
    "\t\tloss.append(((infected[T[idx+1]]-I1)**2)**0.5)\n",
    "\t\talpha_update = lr*(infected[T[idx+1]]-I1)*jacobian_mat[2,0]\n",
    "\t\tbeta_update = lr*(infected[T[idx+1]]-I1)*jacobian_mat[2,1]\n",
    "\t\tgamma1_update = 0 #lr*(infected[T[idx+1]]-I1)*jacobian_mat[2,2]\n",
    "\t\tgamma2_update = 0 #lr*(infected[idx+1]-I1)*jacobian_mat[2,3]\n",
    "\t\talpha_new = alpha+alpha_update\n",
    "\t\tbeta_new = beta+beta_update\n",
    "\t\tgamma1_new = gamma1+gamma1_update\n",
    "\t\tgamma2_new = gamma2+gamma2_update\n",
    "\t\tupdate_alpha.append(alpha_update)\n",
    "\t\tupdate_beta.append(beta_update)\n",
    "\t\tupdate_gamma1.append(gamma1_update)\n",
    "\t\tupdate_gamma2.append(gamma2_update)\n",
    "\t# return np.stack([loss,alpha_vals_S,alpha_vals_E,alpha_vals_I])\n",
    "\treturn np.stack([loss,update_alpha,update_beta,update_gamma1,update_gamma2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_df = pd.read_csv('data/model_param_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_model_param_df = model_param_df[model_param_df['alpha'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = null_model_param_df['Region'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province,country = fetch_province_country_by_region(regions[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for region in regions:\n",
    "    try:\n",
    "        province,country =fetch_province_country_by_region(region)\n",
    "        print(province,country)\n",
    "        region_idx = getIndexByRegion(province,country,time_series_confirmed)\n",
    "        print(region_idx)\n",
    "        region_population = int(getPopulationByRegion(province,country,population_df))\n",
    "        print(region_population)\n",
    "        time_series_start,time_series_end = getDatesByRegion(region,model_param_df)\n",
    "        print(getDatesByRegion(region,model_param_df))\n",
    "        data = time_series_confirmed\n",
    "        gt_infected = np.array(data.iloc[region_idx,time_series_start:time_series_end]).astype(int)\n",
    "        plt.figure(count)\n",
    "#         plt.plot(gt_infected); plt.show()\n",
    "        fit_to_data_population(corona_seir_model_population,gt_infected,region_population,region,count,model_param_df)\n",
    "        count+=1\n",
    "    except:\n",
    "        print(\"No data found for %s,%s\"%(province,country))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
