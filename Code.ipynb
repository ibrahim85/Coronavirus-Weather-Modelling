{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime,timedelta\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "from SEIR import corona_seir_model_population\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'VJga0fAn'\n",
    "blacklist = []\n",
    "retry_set = set()\n",
    "STATIONS = pd.read_csv('data/stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_vals(df,axis='both',subset=[]):\n",
    "    '''\n",
    "    Drops columns with all\n",
    "    nan values from a given \n",
    "    data frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame for which\n",
    "        columns are to be\n",
    "        dropped.\n",
    "        \n",
    "    axis : str\n",
    "        Drops all rows with\n",
    "        nan if axis=rows,\n",
    "        all columns if axis=columns,\n",
    "        and both if axis=both.\n",
    "        \n",
    "    subset : list of str\n",
    "        For all columns in\n",
    "        subset, remove the\n",
    "        NaN rows.\n",
    "    '''\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    assert(isinstance(axis,str))\n",
    "    assert(isinstance(subset,list))\n",
    "    assert(isinstance(col,str) for col in subset)\n",
    "    \n",
    "    df = df.dropna(subset=subset)\n",
    "    \n",
    "    if(axis=='rows'):\n",
    "        df = df.dropna(how='all',axis=0)\n",
    "    elif(axis=='columns'):\n",
    "        df = df.dropna(how='all',axis=1)\n",
    "    elif(axis=='both'):\n",
    "        df = df.dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getWeatherData(latitude,longitude,start_date,end_date):\n",
    "    '''\n",
    "    Returns temperature \n",
    "    and humidity data \n",
    "    as per the latitude \n",
    "    and longitude entered.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "    \n",
    "#     station = getNearbyStation(latitude,longitude)\n",
    "#     if(station is None):\n",
    "#         return\n",
    "# #     else:\n",
    "# #         print(\"Station found for (%f,%f):\\n%s\"%(latitude,longitude,station))\n",
    "#     url = createHourlyURL(station,start_date,end_date)\n",
    "#     weather_data = dict()\n",
    "#     temperature = []\n",
    "#     humidity = []\n",
    "#     print(\"URL for getting weather data for (%f,%f):\\n%s\"%(latitude,longitude,url))\n",
    "#     response = requests.get(url)\n",
    "#     code = response.status_code\n",
    "#     print(\"Got response, status = %f\"%(code))\n",
    "#     try:\n",
    "#         df=pd.DataFrame(response.json()['data'])\n",
    "#         date_list = list(df['time_local'].unique())\n",
    "# #         print(retry_set)\n",
    "#         retry_set.discard((latitude,longitude))\n",
    "#         for date in date_list:\n",
    "#             mean_temp=df[df['time_local']==date]['temperature'].mean()\n",
    "#             mean_humidity=df[df['time_local']==date]['humidity'].mean()\n",
    "#             df_dict = dict()\n",
    "#             df_dict['temperature'] = mean_temp\n",
    "#             df_dict['humidity'] = mean_humidity\n",
    "            \n",
    "#             temperature.append(mean_temp)\n",
    "#             humidity.append(mean_humidity)\n",
    "            \n",
    "#             weather_data[date] = df_dict\n",
    "#     except:\n",
    "#         print('No data found for (%f,%f)'%(latitude,longitude))\n",
    "#         print(\"Retry has to be done for (%f,%f)\"%(latitude,longitude))\n",
    "#         coordinates = (latitude,longitude)\n",
    "#         retry_set.add(coordinates)\n",
    "    \n",
    "#     weather_data['temperature'] = sum(temperature)/len(temperature)\n",
    "#     weather_data['humidity'] = sum(humidity)/len(humidity)\n",
    "#     return weather_data\n",
    "\n",
    "    start_date = datetime.strptime(start_date,'%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date,'%Y-%m-%d')\n",
    "    \n",
    "    day_count = (end_date - start_date).days + 1\n",
    "    temperature_list = []\n",
    "    humidity_list = []\n",
    "    \n",
    "    for single_date in (start_date + timedelta(n) for n in range(day_count)):\n",
    "        epoch_time = int(single_date.timestamp())\n",
    "        url = 'https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/'+str(latitude)+','+str(longitude)+','+str(epoch_time)+'?exclude=currently,flags,minutely'\n",
    "#         print(\"URL to get data: \",url)\n",
    "        response = requests.get(url)\n",
    "        lst = response.json()['hourly']['data']\n",
    "        temp_list = []\n",
    "        for l in lst:\n",
    "            temp_list.append(l['temperature'])\n",
    "    \n",
    "        mean_day_temperature = sum(temp_list)/len(temp_list)\n",
    "        temperature_list.append(mean_day_temperature)\n",
    "        humidity_list.append(response.json()['daily']['data'][0]['humidity'])\n",
    "        \n",
    "    return sum(temperature_list)/len(temperature_list),sum(humidity_list)/len(humidity_list)\n",
    "\n",
    "def getNearbyStation(latitude,longitude):\n",
    "    '''\n",
    "    Given the latitude and\n",
    "    longitude of a area,\n",
    "    returns the nearest station\n",
    "    to it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "\n",
    "    url = createStationURL(latitude,longitude)\n",
    "\n",
    "    try:\n",
    "        filter1 = stations['Latitude']==latitude\n",
    "        filter2 = stations['Longitude']==longitude\n",
    "        return stations.where(filter1 & filter2).dropna(how='all')['Weather Station ID'][0]\n",
    "    except:\n",
    "        try:\n",
    "            print('Station does not exist in \"Stations.csv\", fetching from api...')\n",
    "            response = requests.get(url)\n",
    "#             time.sleep(5)\n",
    "            code = response.status_code\n",
    "            result = response.json()['data'][0]['id']\n",
    "            df = pd.DataFrame({\"Weather Station ID\":[result], \"Latitude\":[latitude], \"Longitude\":[longitude]})\n",
    "            STATIONS.append(df, ignore_index=True)\n",
    "            print('Station fetched for (%f,%f) is: %s'%(latitude,longitude,result))\n",
    "            return result\n",
    "        except Exception as ex:\n",
    "            print('No station found for (%f,%f). URL: %s'%(latitude,longitude,url))\n",
    "            print('exception: ',ex)\n",
    "            if(code == 403):\n",
    "                print(\"Retry has to be done for (%f,%f)\"%(latitude,longitude))\n",
    "                retry_set.add((latitude,longitude))\n",
    "            return\n",
    "        \n",
    "def createStationURL(latitude,longitude):\n",
    "    '''\n",
    "    Returns station URL\n",
    "    for given latitude\n",
    "    and longitude.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "    \n",
    "#     return 'https://api.meteostat.net/v1/stations/nearby?lat='+str(latitude)+'&lon='+str(longitude)+'&limit=1&key='+API_KEY\n",
    "    return 'https://api.meteostat.net/v1/stations/nearby?lat='+str(latitude)+'&lon='+str(longitude)+'&limit=1&key='+API_KEY\n",
    "\n",
    "def createHourlyURL(station_id,start_date,end_date):\n",
    "    '''\n",
    "    Creates weather URL\n",
    "    for given station,\n",
    "    start date and end\n",
    "    date.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    station_id : str\n",
    "        Station id of the\n",
    "        region for which\n",
    "        data is to be fetched\n",
    "        \n",
    "    start_date : str\n",
    "        Date starting from which\n",
    "        data is to be fetched.\n",
    "        \n",
    "    end_date : str\n",
    "        Date ending at which\n",
    "        data is to be fetched\n",
    "    '''\n",
    "    assert(isinstance(station_id,str))\n",
    "    assert(isinstance(start_date,str))\n",
    "    assert(isinstance(end_date,str))\n",
    "    \n",
    "    url = 'https://api.meteostat.net/v1/history/hourly?station='+station_id+'&start='+start_date+'&end='+end_date+'&time_zone=Europe/London&time_format=Y-m-d&key='+API_KEY\n",
    "\n",
    "    return url\n",
    "\n",
    "def getCompleteWeatherData(coordinate_list,complete_weather_data=dict(),flag=True):\n",
    "    '''\n",
    "    Returns consolidated weather\n",
    "    data for all the coordinates\n",
    "    in list of coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coordinate_list : list(tuple)\n",
    "        List of coordinates to\n",
    "        be evaluated.\n",
    "        \n",
    "    complete_weather_data : dict\n",
    "        Map of coordinate to\n",
    "        weather data.\n",
    "    \n",
    "    flag : bool\n",
    "        If True, means some\n",
    "       coordinate_listts failed, retry\n",
    "        the failed requests.\n",
    "    '''\n",
    "    assert(isinstance(flag,bool))\n",
    "    assert(isinstance(complete_weather_data,dict))\n",
    "    assert(isinstance(coordinate_list,list))\n",
    "    \n",
    "    try:\n",
    "        if(flag==True):\n",
    "            flag=False\n",
    "            for lat_long in lat_long_list:\n",
    "                coordinate = (str(lat_long[0]),str(lat_long[1]))\n",
    "                if((coordinate not in complete_weather_data) and (coordinate not in blacklist)):\n",
    "                    flag = True\n",
    "                    complete_weather_data[(str(lat_long[0]),str(lat_long[1]))] = getWeatherData(lat_long[0],lat_long[1])\n",
    "                    time.sleep(10)\n",
    "            return getCompleteWeatherData(coordinate_list,complete_weather_data,flag)\n",
    "        else:\n",
    "            return complete_weather_data\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return complete_weather_data\n",
    "\n",
    "def getIndexByRegion(province,country,df):\n",
    "    '''\n",
    "    Gets index of a region\n",
    "    from a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        index is to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        index is to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        index is to be fetched\n",
    "    '''\n",
    "    assert(isinstance(country,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    if(type(province)!=str and np.isnan(province)):\n",
    "        idx = df[(df['Province/State'].isnull()) & (df['Country/Region']==country)].index\n",
    "        return idx.to_list()[0]\n",
    "    else:\n",
    "        idx = df[(df['Province/State']==province) & (df['Country/Region']==country)].index\n",
    "        return idx.to_list()[0]\n",
    "\n",
    "def fetch_province_country_by_region(region):\n",
    "    '''\n",
    "    Given a region as\n",
    "    Province,Country,\n",
    "    returns the province\n",
    "    and country or just\n",
    "    the country if no province\n",
    "    of the region.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : str\n",
    "        Region to be parsed\n",
    "    '''\n",
    "    assert(isinstance(region,str))\n",
    "    \n",
    "    result = region.split(\",\")\n",
    "    if(len(result)==2):\n",
    "        return result[0],result[1].strip()\n",
    "    else:\n",
    "        return np.NaN,region\n",
    "    \n",
    "def getPopulationByRegion(province,country,df):\n",
    "    '''\n",
    "    Given the province and\n",
    "    country of a region,\n",
    "    fetches the population\n",
    "    from the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        population is to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        population is to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        population is to be fetched\n",
    "    \n",
    "    '''\n",
    "    assert(isinstance(country,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    if(type(province)!=str and np.isnan(province)):\n",
    "        population = df[(df['Province/State'].isnull()) & (df['Country/Region']==country)].Population\n",
    "        return population.to_list()[0]\n",
    "    else:\n",
    "        population = df[(df['Province/State']==province) & (df['Country/Region']==country)].Population\n",
    "        return population.to_list()[0]\n",
    "    \n",
    "def getDatesByRegion(region,df):\n",
    "    '''\n",
    "    Given the province and\n",
    "    country of a region,\n",
    "    fetches the infection\n",
    "    duration from the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        dates are to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        dates are to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        dates are to be fetched\n",
    "    '''\n",
    "    assert(isinstance(region,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    region_row = df[df['Region']==region]\n",
    "    start_date = region_row['Time series start'].to_list()[0]\n",
    "    stop_date = region_row['Time series end'].to_list()[0]\n",
    "    \n",
    "    return start_date,stop_date\n",
    "\n",
    "def getLearningRate(region,df):\n",
    "    '''\n",
    "    Given the province and\n",
    "    country of a region,\n",
    "    fetches the learning\n",
    "    rate from the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    province : str\n",
    "        Province for which\n",
    "        learning rate is\n",
    "        to be fetched\n",
    "        \n",
    "    Country : str\n",
    "        Country for which\n",
    "        learning rate is\n",
    "        to be fetched\n",
    "        \n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which\n",
    "        learning rate is\n",
    "        to be fetched\n",
    "    '''\n",
    "    assert(isinstance(region,str))\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    \n",
    "    region_row = df[df['Region']==region]\n",
    "    learning_rate = region_row['lr'].to_list()\n",
    "#     print(\"Learning rate:\",learning_rate)\n",
    "    if(math.isnan(learning_rate[0])):\n",
    "#         print(\"learning rate is null. returning 0.0004\")\n",
    "        return 0.0004\n",
    "    else:\n",
    "        return learning_rate[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. covid_19_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sno - Serial number<br/>\n",
    "ObservationDate - Date of the observation in MM/DD/YYYY<br/>\n",
    "Province/State - Province or state of the observation (Could be empty when missing)<br/>\n",
    "Country/Region - Country of observation<br/>\n",
    "Last Update - Time in UTC at which the row is updated for the given province or country. (Not standardised and so please clean before using it)<br/>\n",
    "Confirmed - Cumulative number of confirmed cases till that date<br/>\n",
    "Deaths - Cumulative number of of deaths till that date<br/>\n",
    "Recovered - Cumulative number of recovered cases till that date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. COVID_open_line_list_data.csv and COVID19_line_list_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual level data information\n",
    "<br/><br/>\n",
    "\n",
    "ID<br/>\n",
    "age<br/>\n",
    "sex<br/>\n",
    "city<br/>\n",
    "province<br/>\n",
    "country<br/>\n",
    "wuhan(0)_not_wuhan(1)<br/>\n",
    "latitude<br/>\n",
    "longitude<br/>\n",
    "geo_resolution<br/>\n",
    "date_onset_symptoms<br/>\n",
    "date_admission_hospital<br/>\n",
    "date_confirmation<br/>\n",
    "symptoms<br/>\n",
    "lives_in_Wuhan<br/>\n",
    "travel_history_dates<br/>\n",
    "travel_history_location<br/>\n",
    "reported_market_exposure<br/>\n",
    "additional_information<br/>\n",
    "chronic_disease_binary<br/>\n",
    "chronic_disease<br/>\n",
    "source<br/>\n",
    "sequence_available<br/>\n",
    "outcome<br/>\n",
    "date_death_or_discharge<br/>\n",
    "notes_for_discussion<br/>\n",
    "location<br/>\n",
    "admin3<br/>\n",
    "admin2<br/>\n",
    "admin1<br/>\n",
    "country_new<br/>\n",
    "admin_id<br/>\n",
    "data_moderator_initials<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region wise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_wise_data = drop_null_vals(pd.read_csv('data/covid_19_data.csv'),axis=\"both\")\n",
    "region_wise_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_line_list = drop_null_vals(pd.read_csv('data/COVID19_open_line_list.csv'),axis='both')\n",
    "open_line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list = drop_null_vals(pd.read_csv('data/COVID19_line_list_data.csv'),'both')\n",
    "line_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data (John Hopkins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "#     confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Confirmed_archived_0325.csv'\n",
    "#     deaths_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "    deaths_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Deaths_archived_0325.csv'\n",
    "#     recovered_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "    recovered_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Recovered_archived_0325.csv'\n",
    "    \n",
    "    print('Fetching confirmed data from git...')\n",
    "    time_series_confirmed = drop_null_vals(pd.read_csv(confirmed_url,error_bad_lines=False))\n",
    "    print('Fetched confirmed data from git')\n",
    "    \n",
    "    time_series_confirmed.to_csv('data/time_series_covid_19_confirmed.csv')\n",
    "    \n",
    "    time_series_deaths = drop_null_vals(pd.read_csv(deaths_url,error_bad_lines=False))\n",
    "    time_series_deaths.to_csv('data/time_series_covid_19_deaths.csv')\n",
    "    print('Fetched deaths data from git')\n",
    "    \n",
    "    time_series_recovered = drop_null_vals(pd.read_csv(recovered_url,error_bad_lines=False))\n",
    "    time_series_recovered.to_csv('data/time_series_covid_19_recovered.csv')\n",
    "    print('Fetched recovered data from git')\n",
    "    \n",
    "except:\n",
    "    # data not able to be fetched from git, fetching from local system\n",
    "    print(\"Data not able to fetch from git. Using local filesystem.\")\n",
    "    time_series_confirmed = drop_null_vals(pd.read_csv('data/time_series_covid_19_confirmed.csv'))\n",
    "    time_series_deaths = drop_null_vals(pd.read_csv('data/time_series_covid_19_deaths.csv'))\n",
    "    time_series_recovered = drop_null_vals(pd.read_csv('data/time_series_covid_19_recovered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = pd.read_csv(\"data/population_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping today's data\n",
    "time_series_confirmed.drop(time_series_confirmed.columns[len(time_series_confirmed.columns)-1], axis=1, inplace=True)\n",
    "time_series_deaths.drop(time_series_deaths.columns[len(time_series_deaths.columns)-1], axis=1, inplace=True)\n",
    "time_series_recovered.drop(time_series_recovered.columns[len(time_series_recovered.columns)-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_confirmed_coord = drop_null_vals(time_series_confirmed,subset=['Lat','Long'])\n",
    "series_confirmed_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and humidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_list = list(series_confirmed_coord[['Lat','Long']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>API Sample Calls</u></b>\n",
    "\n",
    "API to fetch station: https://api.meteostat.net/v1/stations/nearby?lat=1.283&lon=103.83&limit=1&key=XXXXXXX<br/>\n",
    "API to fetch daily historical data(not used any more): https://api.meteostat.net/v1/history/daily?station=10637&start=2017-01-01&end=2017-12-31&key=XXXXXXXX<br/>\n",
    "API to fetch historical hourly data: https://api.meteostat.net/v1/history/hourly?station=03772&start=2019-05-02&end=2019-05-11&time_zone=Europe/London&time_format=Y-m-d%20H:i&key=XXXXXXXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = str(datetime.strptime(series_confirmed_coord.columns[4], '%m/%d/%y').date())\n",
    "END_DATE = str(datetime.strptime(series_confirmed_coord.columns[-1], '%m/%d/%y').date())\n",
    "complete_weather_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for lat_long in lat_long_list:\n",
    "#     if(str(lat_long[0])+str(lat_long[1]) not in complete_weather_data):\n",
    "#         complete_weather_data[str(lat_long[0])+str(lat_long[1])] = getWeatherData(lat_long[0],lat_long[1])\n",
    "#         time.sleep(60)\n",
    "\n",
    "# complete_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp = getCompleteWeatherData(lat_long_list)\n",
    "# STATIONS.to_csv(r'data\\stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dict = {}\n",
    "# for key in list(temp.keys()):\n",
    "#     new_dict[str(key[0])+\",\"+str(key[1])] = temp[key]\n",
    "    \n",
    "# my_dict = pd.DataFrame(new_dict).dropna(how=\"all\",axis=1).to_dict()\n",
    "# with open('data/weather_mapping_data.json', 'w') as json_file:\n",
    "#     json.dump(my_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_column = time_series_confirmed.columns[len(time_series_confirmed.columns)-1]\n",
    "sorted_series=time_series_confirmed.sort_values(by=last_column,ascending=False)\n",
    "sorted_series = sorted_series[sorted_series[last_column]>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_series.where(sorted_series['Country/Region'] == country,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_data_population(model,gt_infected,population,region,count,df):\n",
    "\tT_max = gt_infected.shape[0]-1\n",
    "\tmax_infected = np.max(gt_infected)\n",
    "\tdt = 1\n",
    "\tT = np.linspace(0,T_max,int(T_max/dt)+1).astype(int)\n",
    "\tN = population\n",
    "\tinit_exposed = int(gt_infected[0]*2)\n",
    "\tinit_vals = (N-init_exposed-gt_infected[0]),init_exposed,gt_infected[0],0\n",
    "\tgamma2 = 0.03\n",
    "\trho = 1.0\n",
    "\ttotal_epochs = 20000\n",
    "\t# lr = 0.04\n",
    "\t# lrd = 0.01\n",
    "# \tlr = 0.0004/max_infected\n",
    "\tlr = getLearningRate(region,df)/max_infected\n",
    "# \tprint(\"learning rate: \",lr)\n",
    "\tlrd = 0.000\n",
    "\tcurr_params = 0.2,0.5,0.0,gamma2\n",
    "\tloss_arr = []\n",
    "\talpha_arr = []\n",
    "\tbeta_arr = []\n",
    "\tgamma1_arr = []\n",
    "\tgamma2_arr = []\n",
    "\tfor epoch in tqdm(range(total_epochs)):\n",
    "\t\tcurr_lr = lr/(1+epoch*lrd)\n",
    "\t\tloss_jacobian = epoch_fit_params_corona_seir_population(init_vals,curr_params,T,gt_infected,lr=curr_lr)\n",
    "\t\tloss_epoch = np.sum(loss_jacobian[0])\n",
    "\t\tnew_alpha = max(0,curr_params[0]+np.sum(loss_jacobian[1]))\n",
    "\t\tnew_beta = max(0,curr_params[1]+np.sum(loss_jacobian[2]))\n",
    "\t\tnew_gamma1 = max(0,curr_params[2]+np.sum(loss_jacobian[3]))\n",
    "\t\tnew_gamma2 = max(0,curr_params[3]+np.sum(loss_jacobian[4]))\n",
    "\t\tcurr_params = new_alpha,new_beta,new_gamma1,new_gamma2\n",
    "\t\tloss_arr.append(loss_epoch)\n",
    "\t\talpha_arr.append(new_alpha)\n",
    "\t\tbeta_arr.append(new_beta)\n",
    "\t\tgamma1_arr.append(new_gamma1)\n",
    "\t\tgamma2_arr.append(new_gamma2)\n",
    "\tbest_epoch = np.argmin(np.array(loss_arr))\n",
    "\tprint(\"Best learned params: {} {} {}\".format(alpha_arr[best_epoch],beta_arr[best_epoch],gamma1_arr[best_epoch]))\n",
    "\tplt.figure(count)\n",
    "\tplt.subplot(221)\n",
    "\tplt.axvline(x=best_epoch,color='k',linestyle='--')\n",
    "\tplt.plot(list(range(total_epochs)),loss_arr)\n",
    "\tplt.ylabel('Total MSE loss')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.subplot(222)\n",
    "\tplt.axvline(x=best_epoch,color='k',linestyle='--')\n",
    "\tplt.plot(list(range(total_epochs)),alpha_arr,label='alpha')\n",
    "\tplt.plot(list(range(total_epochs)),beta_arr,label='beta')\n",
    "\tplt.plot(list(range(total_epochs)),gamma1_arr,label='gamma1')\n",
    "\t# plt.plot(list(range(total_epochs)),gamma2_arr,label='gamma2')\n",
    "\tplt.title('Learning trajectory for '+region)\n",
    "\tplt.ylabel('Parameter value')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.legend()\n",
    "\t# print(init_vals)\n",
    "\tbest_params = alpha_arr[best_epoch],beta_arr[best_epoch],gamma1_arr[best_epoch],gamma2\n",
    "\tT_pred = np.linspace(0,10+T_max,int((10+T_max)/dt)+1).astype(int)\n",
    "\tlearned_results = model(init_vals,best_params,T_pred)\n",
    "\t# plt.figure(2)\n",
    "\tplt.subplot(212)\n",
    "\t# p = plt.plot(T,sim_results[0],label='GT Susceptible')\n",
    "\t# p = plt.plot(T_pred,learned_results[0]/N,linestyle='--',label='Predicted Susceptible')\n",
    "\t# p = plt.plot(T,sim_results[1],label='GT Exposed')\n",
    "\t# p = plt.plot(T_pred,learned_results[1]/N,linestyle='--',label='Predicted Exposed')\n",
    "\tp = plt.plot(gt_infected[T]/N,label='GT Infected')\n",
    "\tplt.plot(T_pred,learned_results[2]/N,color=p[0].get_color(),linestyle='--',label='Predicted Infected')\n",
    "\tprint(\"error percentage : \",region,abs(100*(learned_results[2][len(gt_infected)-1]-gt_infected[-1])/gt_infected[-1]))\n",
    "\t# p = plt.plot(T,sim_results[3],label='Recovered')\n",
    "\tp = plt.plot(T_pred,learned_results[3]/N,linestyle='--',label='Predicted Recovered')\n",
    "\tplt.legend()\n",
    "\tplt.ylabel('Fraction of population')\n",
    "\tplt.xlabel('Time (days)')\n",
    "\tplt.title('GT and learned models')\n",
    "\tplt.show()\n",
    "    \n",
    "def epoch_fit_params_corona_seir_population(init_vals, init_params, T, infected, lr=1e-2):\n",
    "\tmax_infected = np.max(infected)\n",
    "\tS0,E0,I0,R0 = init_vals\n",
    "\tN = S0+E0+I0+R0\n",
    "\tS, E, I , R = [S0], [E0], [I0], [R0]\n",
    "\tloss = [0]\n",
    "\talpha, beta, gamma1, gamma2 = init_params\n",
    "\tdt = T[1]-T[0]\n",
    "\tjacobian_mat = np.zeros((3,4))\n",
    "\t# updated_alpha, updated_beta, updated_gamma1, updated_gamma2 = [alpha],[beta],[gamma1],[gamma2]\n",
    "\tupdate_alpha, update_beta, update_gamma1, update_gamma2 = [0],[0],[0],[0]\n",
    "\t# print(infected)\n",
    "\tfor idx,t in enumerate(T[1:-3]):\n",
    "\t\t# print(\"{} {} {} {} {:.3f} {:.3f} {:.3f} {:3.3f}\".format(t,alpha,beta,gamma1,S[-1],E[-1],I[-1],np.max(jacobian_mat)))\n",
    "\t\tupdate_mat = np.array([[(1-beta*E[-1]/N),(-beta*S[-1]/N),0],[(beta*E[-1]/N),(1+beta*S[-1]/N-alpha-gamma1),0],[0,alpha,(1-gamma2)]])\n",
    "\t\tadd_mat = np.array([[0,(-S[-1]*(E[-1]/N)),0,0],[(-E[-1]),S[-1]*(E[-1]/N),(-E[-1]),0],[E[-1],0,0,(-I[-1])]])/max_infected\n",
    "\t\tjacobian_mat = np.matmul(update_mat,jacobian_mat)+add_mat\n",
    "\t\tmin_val = N\n",
    "\t\tS1 = S[-1] - (beta*S[-1]*E[-1]/N)*dt\n",
    "\t\tif S1<min_val: min_val=S1\n",
    "\t\tE1 = E[-1] + (beta*S[-1]*E[-1]/N - alpha*E[-1] - gamma1*E[-1])*dt\n",
    "\t\tif E1<min_val: min_val=E1\n",
    "\t\tI1 = I[-1] + (alpha*E[-1] - gamma2*I[-1])*dt\n",
    "\t\tif I1<min_val: min_val=I1\n",
    "\t\tR1 = R[-1] + (gamma1*E[-1] + gamma2*I[-1])*dt\n",
    "\t\tif R1<min_val: min_val=R1\n",
    "\t\tN1 = S1+E1+I1+R1-4*min_val\n",
    "\t\tS1 = N*(S1-min_val)/N1\n",
    "\t\tE1 = N*(E1-min_val)/N1\n",
    "\t\tI1 = N*(I1-min_val)/N1\n",
    "\t\tR1 = N*(R1-min_val)/N1\n",
    "\t\tS.append(S1)\n",
    "\t\tE.append(E1)\n",
    "\t\tI.append(I1)\n",
    "\t\tR.append(R1)\n",
    "\t\t# print(T[idx+1],infected[T[idx+1]])\n",
    "\t\tloss.append(((infected[T[idx+1]]-I1)**2)**0.5)\n",
    "\t\talpha_update = lr*(infected[T[idx+1]]-I1)*jacobian_mat[2,0]\n",
    "\t\tbeta_update = lr*(infected[T[idx+1]]-I1)*jacobian_mat[2,1]\n",
    "\t\tgamma1_update = 0 #lr*(infected[T[idx+1]]-I1)*jacobian_mat[2,2]\n",
    "\t\tgamma2_update = 0 #lr*(infected[idx+1]-I1)*jacobian_mat[2,3]\n",
    "\t\talpha_new = alpha+alpha_update\n",
    "\t\tbeta_new = beta+beta_update\n",
    "\t\tgamma1_new = gamma1+gamma1_update\n",
    "\t\tgamma2_new = gamma2+gamma2_update\n",
    "\t\tupdate_alpha.append(alpha_update)\n",
    "\t\tupdate_beta.append(beta_update)\n",
    "\t\tupdate_gamma1.append(gamma1_update)\n",
    "\t\tupdate_gamma2.append(gamma2_update)\n",
    "\t# return np.stack([loss,alpha_vals_S,alpha_vals_E,alpha_vals_I])\n",
    "\treturn np.stack([loss,update_alpha,update_beta,update_gamma1,update_gamma2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_df = pd.read_csv('data/model_param_results.csv')\n",
    "null_model_param_df = model_param_df[model_param_df['alpha'].isna()]\n",
    "regions = null_model_param_df['Region'].to_list()\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for region in regions:\n",
    "    try:\n",
    "        province,country =fetch_province_country_by_region(region)\n",
    "#         print(province,country)\n",
    "        region_idx = getIndexByRegion(province,country,time_series_confirmed)\n",
    "#         print(region_idx)\n",
    "        region_population = int(getPopulationByRegion(province,country,population_df))\n",
    "#         print(region_population)\n",
    "        time_series_start,time_series_end = getDatesByRegion(region,model_param_df)\n",
    "#         print(getDatesByRegion(region,model_param_df))\n",
    "        data = time_series_confirmed\n",
    "        gt_infected = np.array(data.iloc[region_idx,time_series_start:time_series_end]).astype(int)\n",
    "        plt.figure(count)\n",
    "#         plt.plot(gt_infected); plt.show()\n",
    "        fit_to_data_population(corona_seir_model_population,gt_infected,region_population,region,count,model_param_df)\n",
    "        count+=1\n",
    "    except:\n",
    "        print(\"No data found for %s,%s\"%(province,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getWeatherData(43.000000,12.000000,'2/23/2020','3/21/2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "for region in model_param_df['Region']:\n",
    "    try:\n",
    "        print(region)\n",
    "        province,country =fetch_province_country_by_region(region)\n",
    "        region_idx = getIndexByRegion(province,country,time_series_confirmed)\n",
    "        latitude = time_series_confirmed.loc[region_idx]['Lat']\n",
    "        longitude = time_series_confirmed.loc[region_idx]['Long']\n",
    "        df = model_param_df[model_param_df['Region']==region]\n",
    "        start_date_index = df['Time series start'].to_list()[0]\n",
    "        start_date = str(datetime.strptime(time_series_confirmed.columns[start_date_index], '%m/%d/%Y').date())\n",
    "        end_date_index = df['Time series end'].to_list()[0]\n",
    "        end_date = str(datetime.strptime(time_series_confirmed.columns[end_date_index-1], '%m/%d/%Y').date())\n",
    "        print('latitude,longitude=',str(latitude)+\",\"+str(longitude))\n",
    "        print('start date= \"',start_date,end='\"\\n')\n",
    "        print('end date= \"',end_date,end='\"\\n')\n",
    "#         if(region not in result.keys()):\n",
    "#             result[region] = getWeatherData(latitude,longitude,start_date,end_date)\n",
    "#         time.sleep(30)\n",
    "    except:\n",
    "        print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.937961309523814, 0.6532142857142855)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Italy\n",
    "getWeatherData(43.0,12.0,'2020-02-23','2020-03-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583222400?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583308800?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583395200?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583481600?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583568000?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583654400?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583737200?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583823600?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583910000?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1583996400?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584082800?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584169200?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584255600?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584342000?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584428400?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584514800?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584601200?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584687600?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584774000?exclude=currently,flags,minutely\n",
      "URL to get data:  https://api.darksky.net/forecast/f331ae03e10920ae370dc361d5acc7fd/40.0,-4.0,1584860400?exclude=currently,flags,minutely\n"
     ]
    }
   ],
   "source": [
    "#Spain\n",
    "temp,hum = getWeatherData(40.0,-4.0,'2020-03-03','2020-03-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.86229166666668, 0.8140000000000001)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Germany\n",
    "# latitude,longitude: 51.0 9.0\n",
    "# start date:  2020-03-03\n",
    "# end date:  2020-03-22\n",
    "\n",
    "getWeatherData(51.0,9.0,'2020-03-03','2020-03-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.373577251552796, 0.2342857142857143)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iran\n",
    "latitude,longitude= 32.0,53.0\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.59130952380952, 0.7792857142857145)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# France\n",
    "latitude,longitude= 46.2276,2.2137\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.63215476190475, 0.6037142857142856)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#South Korea\n",
    "latitude,longitude = 36.0,128.0\n",
    "start_date = '2020-02-09'\n",
    "end_date = '2020-03-14'\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.0967857142857, 0.8496428571428571)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Switzerland\n",
    "latitude,longitude = 46.8182,8.2275\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.13974702380953, 0.9060714285714286)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UK\n",
    "latitude,longitude = 55.3781,-3.436\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43.53410714285714, 0.7985714285714286)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Netherlands\n",
    "latitude,longitude = 52.1326,5.2913\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.746651785714285, 0.7871428571428571)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Belgium\n",
    "latitude,longitude = 50.8333,4.0\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.41992559523809, 0.7092857142857143)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Austria\n",
    "latitude,longitude = 47.5162,14.5501\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.640773809523807, 0.6539285714285714)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Norway\n",
    "latitude,longitude = 60.472,8.4689\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26.660803571428573, 0.6921428571428571)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sweden\n",
    "latitude,longitude = 63.0,16.0\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.83046130952381, 0.7321428571428573)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Portugal\n",
    "latitude,longitude = 39.3999,-8.2245\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.83269345238098, 0.899642857142857)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Brazil\n",
    "latitude,longitude = -14.235,-51.9253\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57.52429166666667, 0.7110000000000001)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guangdong, China\n",
    "latitude,longitude = 23.3417,113.4244\n",
    "start_date= \"2020-01-23\"\n",
    "end_date= \"2020-02-11\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.42535714285714, 0.8207142857142857)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Denmark\n",
    "latitude,longitude = 56.2639,9.5018\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75.50111607142857, 0.9764285714285712)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Malaysia\n",
    "latitude,longitude = 2.5,112.5\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.39342105263158, 0.6373684210526317)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Henan,China\n",
    "latitude,longitude = 33.882,113.614\n",
    "start_date= \"2020-01-24\"\n",
    "end_date= \"2020-02-11\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.78515350877193, 0.8494736842105262)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zhejiang,China\n",
    "latitude,longitude = 29.1832,120.0934\n",
    "start_date= \"2020-01-22\"\n",
    "end_date= \"2020-02-09\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.62133928571428, 0.7342857142857142)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Czechia\n",
    "latitude,longitude = 49.8175,15.473\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.80682432432432, 0.62)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Japan\n",
    "latitude,longitude = 36.0,138.0\n",
    "start_date= \"2020-02-16\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55.64596726190475, 0.7028571428571427)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Israel\n",
    "latitude,longitude = 31.0,35.0\n",
    "start_date= \"2020-02-25\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43.197653508771936, 0.8026315789473685)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hunan,China\n",
    "latitude,longitude = 27.6104,111.7088\n",
    "start_date= \"2020-01-22\"\n",
    "end_date= \"2020-02-09\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.48799818840579, 0.572)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New York, US\n",
    "latitude,longitude = 40.7128,-74.006\n",
    "start_date= \"2020-03-04\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46.126928442028984, 0.7149999999999999)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Washington, US\n",
    "latitude,longitude = 45.547,-123.1386\n",
    "start_date= \"2020-03-04\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.33351539855073, 0.5875)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New Jersey, US\n",
    "latitude,longitude = 40.2989,-74.521\n",
    "start_date= \"2020-03-04\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55.67338224637681, 0.6755000000000001)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# California, US\n",
    "latitude,longitude = 36.1162,-119.6816\n",
    "start_date= \"2020-03-04\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.04840217391304, 0.724)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Illinois, US\n",
    "latitude,longitude = 40.3495,-88.9861\n",
    "start_date= \"2020-03-04\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.26401983218917, 0.6684210526315789)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Michigan, US\n",
    "latitude,longitude = 43.3266,-84.5361\n",
    "start_date= \"2020-03-05\"\n",
    "end_date= \"2020-03-23\"\n",
    "\n",
    "getWeatherData(latitude,longitude,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.418125, 0.6783333333333333)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hubei, China\n",
    "start_date= \"2020-01-26\"\n",
    "end_date= \"2020-02-12\"\n",
    "\n",
    "getWeatherData(30.9756, 112.2707,start_date,end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
