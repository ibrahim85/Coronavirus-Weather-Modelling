{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'VJga0fAn'\n",
    "blacklist = []\n",
    "retry_set = set()\n",
    "STATIONS = pd.read_csv('data/stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_vals(df,axis='both',subset=[]):\n",
    "    '''\n",
    "    Drops columns with all\n",
    "    nan values from a given \n",
    "    data frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame for which\n",
    "        columns are to be\n",
    "        dropped.\n",
    "        \n",
    "    axis : str\n",
    "        Drops all rows with\n",
    "        nan if axis=rows,\n",
    "        all columns if axis=columns,\n",
    "        and both if axis=both.\n",
    "        \n",
    "    subset : list of str\n",
    "        For all columns in\n",
    "        subset, remove the\n",
    "        NaN rows.\n",
    "    '''\n",
    "    assert(isinstance(df,pd.DataFrame))\n",
    "    assert(isinstance(axis,str))\n",
    "    assert(isinstance(subset,list))\n",
    "    assert(isinstance(col,str) for col in subset)\n",
    "    \n",
    "    df = df.dropna(subset=subset)\n",
    "    \n",
    "    if(axis=='rows'):\n",
    "        df = df.dropna(how='all',axis=0)\n",
    "    elif(axis=='columns'):\n",
    "        df = df.dropna(how='all',axis=1)\n",
    "    elif(axis=='both'):\n",
    "        df = df.dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getWeatherData(latitude,longitude):\n",
    "    '''\n",
    "    Returns temperature \n",
    "    and humidity data \n",
    "    as per the latitude \n",
    "    and longitude entered.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "    \n",
    "    station = getNearbyStation(latitude,longitude)\n",
    "    if(station is None):\n",
    "        return\n",
    "    else:\n",
    "        print(\"Station found for (%f,%f):\\n%s\"%(latitude,longitude,station))\n",
    "    url = createHourlyURL(station,START_DATE,END_DATE)\n",
    "    weather_data = dict()\n",
    "    print(\"URL for getting weather data for (%f,%f):\\n%s\"%(latitude,longitude,url))\n",
    "    response = requests.get(url)\n",
    "    code = response.status_code\n",
    "    print(\"Got response, status = %f\"%(code))\n",
    "    try:\n",
    "        df=pd.DataFrame(response.json()['data'])\n",
    "        date_list = list(df['time_local'].unique())\n",
    "#         print(retry_set)\n",
    "        retry_set.discard((latitude,longitude))\n",
    "        for date in date_list:\n",
    "            mean_temp=df[df['time_local']==date]['temperature'].mean()\n",
    "            mean_humidity=df[df['time_local']==date]['humidity'].mean()\n",
    "            df_dict = dict()\n",
    "            df_dict['temperature'] = mean_temp\n",
    "            df_dict['humidity'] = mean_humidity\n",
    "            weather_data[date] = df_dict\n",
    "    except:\n",
    "        print('No data found for (%f,%f)'%(latitude,longitude))\n",
    "        print(\"Retry has to be done for (%f,%f)\"%(latitude,longitude))\n",
    "        coordinates = (latitude,longitude)\n",
    "        retry_set.add(coordinates)\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "def getNearbyStation(latitude,longitude):\n",
    "    '''\n",
    "    Given the latitude and\n",
    "    longitude of a area,\n",
    "    returns the nearest station\n",
    "    to it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "\n",
    "    url = createStationURL(latitude,longitude)\n",
    "\n",
    "    try:\n",
    "        filter1 = stations['Latitude']==latitude\n",
    "        filter2 = stations['Longitude']==longitude\n",
    "        return stations.where(filter1 & filter2).dropna(how='all')['Weather Station ID'][0]\n",
    "    except:\n",
    "        try:\n",
    "            print('Station does not exist in \"Stations.csv\", fetching from api...')\n",
    "            response = requests.get(url)\n",
    "            code = response.status_code\n",
    "            result = response.json()['data'][0]['id']\n",
    "            df = pd.DataFrame({\"Weather Station ID\":[result], \"Latitude\":[latitude], \"Longitude\":[longitude]})\n",
    "            STATIONS.append(df, ignore_index=True)\n",
    "            print('Station fetched for (%f,%f) is: %s'%(latitude,longitude,result))\n",
    "            return result\n",
    "        except Exception as ex:\n",
    "            print('No station found for (%f,%f). URL: %s'%(latitude,longitude,url))\n",
    "            print('exception: ',ex)\n",
    "            if(code == 403):\n",
    "                print(\"Retry has to be done for (%f,%f)\"%(latitude,longitude))\n",
    "                retry_set.add((latitude,longitude))\n",
    "            return\n",
    "        \n",
    "def createStationURL(latitude,longitude):\n",
    "    '''\n",
    "    Returns station URL\n",
    "    for given latitude\n",
    "    and longitude.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of region\n",
    "        for fetching the\n",
    "        weather data\n",
    "        \n",
    "    longitude : float\n",
    "        Longitude of region\n",
    "        for fetching weather\n",
    "        data\n",
    "    '''\n",
    "    assert(isinstance(latitude,float))\n",
    "    assert(isinstance(longitude,float))\n",
    "    \n",
    "    return 'https://api.meteostat.net/v1/stations/nearby?lat='+str(latitude)+'&lon='+str(longitude)+'&limit=1&key='+API_KEY\n",
    "\n",
    "def createHourlyURL(station_id,start_date,end_date):\n",
    "    '''\n",
    "    Creates weather URL\n",
    "    for given station,\n",
    "    start date and end\n",
    "    date.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    station_id : str\n",
    "        Station id of the\n",
    "        region for which\n",
    "        data is to be fetched\n",
    "        \n",
    "    start_date : str\n",
    "        Date starting from which\n",
    "        data is to be fetched.\n",
    "        \n",
    "    end_date : str\n",
    "        Date ending at which\n",
    "        data is to be fetched\n",
    "    '''\n",
    "    assert(isinstance(station_id,str))\n",
    "    assert(isinstance(start_date,str))\n",
    "    assert(isinstance(end_date,str))\n",
    "    \n",
    "    url = 'https://api.meteostat.net/v1/history/hourly?station='+station_id+'&start='+start_date+'&end='+end_date+'&time_zone=Europe/London&time_format=Y-m-d&key='+API_KEY\n",
    "\n",
    "    return url\n",
    "\n",
    "def getCompleteWeatherData(coordinate_list,complete_weather_data=dict(),flag=True):\n",
    "    '''\n",
    "    Returns consolidated weather\n",
    "    data for all the coordinates\n",
    "    in list of coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coordinate_list : list(tuple)\n",
    "        List of coordinates to\n",
    "        be evaluated.\n",
    "        \n",
    "    complete_weather_data : dict\n",
    "        Map of coordinate to\n",
    "        weather data.\n",
    "    \n",
    "    flag : bool\n",
    "        If True, means some\n",
    "       coordinate_listts failed, retry\n",
    "        the failed requests.\n",
    "    '''\n",
    "    assert(isinstance(flag,bool))\n",
    "    assert(isinstance(complete_weather_data,dict))\n",
    "    assert(isinstance(coordinate_list,list))\n",
    "    \n",
    "    try:\n",
    "        if(flag==True):\n",
    "            flag=False\n",
    "            for lat_long in lat_long_list:\n",
    "                coordinate = (str(lat_long[0]),str(lat_long[1]))\n",
    "                if((coordinate not in complete_weather_data) and (coordinate not in blacklist)):\n",
    "                    flag = True\n",
    "                    complete_weather_data[(str(lat_long[0]),str(lat_long[1]))] = getWeatherData(lat_long[0],lat_long[1])\n",
    "                    time.sleep(10)\n",
    "            return getCompleteWeatherData(coordinate_list,complete_weather_data,flag)\n",
    "        else:\n",
    "            return complete_weather_data\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return complete_weather_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. covid_19_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sno - Serial number<br/>\n",
    "ObservationDate - Date of the observation in MM/DD/YYYY<br/>\n",
    "Province/State - Province or state of the observation (Could be empty when missing)<br/>\n",
    "Country/Region - Country of observation<br/>\n",
    "Last Update - Time in UTC at which the row is updated for the given province or country. (Not standardised and so please clean before using it)<br/>\n",
    "Confirmed - Cumulative number of confirmed cases till that date<br/>\n",
    "Deaths - Cumulative number of of deaths till that date<br/>\n",
    "Recovered - Cumulative number of recovered cases till that date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. COVID_open_line_list_data.csv and COVID19_line_list_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual level data information\n",
    "<br/><br/>\n",
    "\n",
    "ID<br/>\n",
    "age<br/>\n",
    "sex<br/>\n",
    "city<br/>\n",
    "province<br/>\n",
    "country<br/>\n",
    "wuhan(0)_not_wuhan(1)<br/>\n",
    "latitude<br/>\n",
    "longitude<br/>\n",
    "geo_resolution<br/>\n",
    "date_onset_symptoms<br/>\n",
    "date_admission_hospital<br/>\n",
    "date_confirmation<br/>\n",
    "symptoms<br/>\n",
    "lives_in_Wuhan<br/>\n",
    "travel_history_dates<br/>\n",
    "travel_history_location<br/>\n",
    "reported_market_exposure<br/>\n",
    "additional_information<br/>\n",
    "chronic_disease_binary<br/>\n",
    "chronic_disease<br/>\n",
    "source<br/>\n",
    "sequence_available<br/>\n",
    "outcome<br/>\n",
    "date_death_or_discharge<br/>\n",
    "notes_for_discussion<br/>\n",
    "location<br/>\n",
    "admin3<br/>\n",
    "admin2<br/>\n",
    "admin1<br/>\n",
    "country_new<br/>\n",
    "admin_id<br/>\n",
    "data_moderator_initials<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region wise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_wise_data = drop_null_vals(pd.read_csv('data/covid_19_data.csv'),axis=\"both\")\n",
    "region_wise_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_line_list = drop_null_vals(pd.read_csv('data/COVID19_open_line_list.csv'),axis='both')\n",
    "open_line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list = drop_null_vals(pd.read_csv('data/COVID19_line_list_data.csv'),'both')\n",
    "line_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data (John Hopkins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv'\n",
    "    deaths_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv'\n",
    "    recovered_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv'\n",
    "    time_series_confirmed = drop_null_vals(pd.read_csv(confirmed_url,error_bad_lines=False))\n",
    "    time_series_confirmed.to_csv('data/time_series_covid_19_confirmed.csv')\n",
    "    \n",
    "    time_series_deaths = drop_null_vals(pd.read_csv(deaths_url,error_bad_lines=False))\n",
    "    time_series_deaths.to_csv('data/time_series_covid_19_deaths.csv')\n",
    "    \n",
    "    time_series_recovered = drop_null_vals(pd.read_csv(recovered_url,error_bad_lines=False))\n",
    "    time_series_recovered.to_csv('data/time_series_covid_19_recovered.csv')\n",
    "except:\n",
    "    # data not able to be fetched from git, fetching from local system\n",
    "    print(\"Data not able to fetch from git. Using local filesystem.\")\n",
    "    time_series_confirmed = drop_null_vals(pd.read_csv('data/time_series_covid_19_confirmed.csv'))\n",
    "    time_series_deaths = drop_null_vals(pd.read_csv('data/time_series_covid_19_deaths.csv'))\n",
    "    time_series_recovered = drop_null_vals(pd.read_csv('data/time_series_covid_19_recovered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping today's data\n",
    "time_series_confirmed.drop(time_series_confirmed.columns[len(time_series_confirmed.columns)-1], axis=1, inplace=True)\n",
    "time_series_deaths.drop(time_series_deaths.columns[len(time_series_deaths.columns)-1], axis=1, inplace=True)\n",
    "time_series_recovered.drop(time_series_recovered.columns[len(time_series_recovered.columns)-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_confirmed_coord = drop_null_vals(time_series_confirmed,subset=['Lat','Long'])\n",
    "series_confirmed_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and humidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_list = list(series_confirmed_coord[['Lat','Long']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>API Sample Calls</u></b>\n",
    "\n",
    "API to fetch station: https://api.meteostat.net/v1/stations/nearby?lat=1.283&lon=103.83&limit=1&key=XXXXXXX<br/>\n",
    "API to fetch daily historical data(not used any more): https://api.meteostat.net/v1/history/daily?station=10637&start=2017-01-01&end=2017-12-31&key=XXXXXXXX<br/>\n",
    "API to fetch historical hourly data: https://api.meteostat.net/v1/history/hourly?station=03772&start=2019-05-02&end=2019-05-11&time_zone=Europe/London&time_format=Y-m-d%20H:i&key=XXXXXXXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = str(datetime.strptime(series_confirmed_coord.columns[4], '%m/%d/%y').date())\n",
    "END_DATE = str(datetime.strptime(series_confirmed_coord.columns[-1], '%m/%d/%y').date())\n",
    "complete_weather_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lat_long in lat_long_list:\n",
    "    if(str(lat_long[0])+str(lat_long[1]) not in complete_weather_data):\n",
    "        complete_weather_data[str(lat_long[0])+str(lat_long[1])] = getWeatherData(lat_long[0],lat_long[1])\n",
    "        time.sleep(60)\n",
    "\n",
    "complete_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = getCompleteWeatherData(lat_long_list)\n",
    "STATIONS.to_csv(r'data\\stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(temp).dropna(how=\"all\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for key in list(temp.keys()):\n",
    "    new_dict[str(key[0])+\",\"+str(key[1])] = temp[key]\n",
    "    \n",
    "my_dict = pd.DataFrame(new_dict).dropna(how=\"all\",axis=1).to_dict()\n",
    "with open('data/weather_mapping_data.json', 'w') as json_file:\n",
    "    json.dump(my_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_column = time_series_confirmed.columns[len(time_series_confirmed.columns)-1]\n",
    "sorted_series=time_series_confirmed.sort_values(by=last_column,ascending=False)\n",
    "sorted_series = sorted_series[sorted_series[last_column]>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
